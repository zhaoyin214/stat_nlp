{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2章：预备知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 概率论基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 概率\n",
    "\n",
    "概率（probability）是从随机试验中的事件到实数域的映射函数。用以表示事件发生的可能性。如果用$P(A)$作为事件$A$的概率，$\\Omega$是试验的样本空间，则概率函数必须满足如下三条公理：\n",
    "\n",
    "公理2-1（非负性）$P(A) \\geq 0$\n",
    "\n",
    "公理2-2（规范性）$P(\\Omega) = 1$\n",
    "\n",
    "公理2-3（可列可加性）对于可列无穷多个事件$A_{1}, A_{2}, \\dots$，如果事件两两互不相容，即对于任意的$i$和$j$（$i \\not = j$），事件$A_{i}$和$A_{j}$不相交（$A_{i} \\bigcap A_{j} = \\emptyset$），则有\n",
    "\n",
    "$$P(\\bigcup_{i = 0}^{\\infty} A_{i}) = \\sum_{i = 0}^{\\infty} P(A_{i}) \\tag{2-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 最大似然估计\n",
    "\n",
    "如果$\\{s_{1}, s_{2}, \\dots, s_{n}\\}$是一个试验的样本空间，在相同的情况下重复试验$N$次，观察到样本$s_{k}$（$1 \\leq k \\leq n$）的次数为$n_{N}(s_{k})$，那么，在这$N$次试验中的相对频率为\n",
    "\n",
    "$$q_{N}(s_{k}) = \\frac{n_{N}(s_{k})}{N} \\tag {2-2}$$\n",
    "\n",
    "由于$\\sum_{k = 1}^{n} n_{N}(s_{k}) = N$，因此，$\\sum_{k = 1}^{n} q_{N}(s_{k}) = 1$\n",
    "\n",
    "$$\\lim_{N \\rightarrow \\infty} q_{N}(s_{k}) = P(s_{k}) \\tag {2-3}$$\n",
    "\n",
    "因此，通常用相对频率作为概率的估计值。这种估计概率值的方法称为最大似然估计（maximum likelihood estimation，MLE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 条件概率\n",
    "\n",
    "如果$A$和$B$是样本空间$\\Omega$上的两个事件，$P(B) \\gt 0$，那么，在给定$B$时$A$的条件概率（conditional probability）$P(A | B)$为\n",
    "\n",
    "$$P(A | B) = \\frac{P(A \\bigcap B)}{P(B)} \\tag{2-4}$$\n",
    "\n",
    "概率乘法定理（乘法规则）\n",
    "\n",
    "$$P(A_{1} \\bigcap \\cdots \\bigcap A_{n}) = P(A_{1}) P(A_{2} | A_{1}) P(A_{3} | A_{1} \\bigcap A_{2}) \\cdots P(A_{n} | \\bigcap_{i = 1}^{n - 1} A_{i}) \\tag {2-6}$$\n",
    "\n",
    "条件概率的三个基本性质：\n",
    "\n",
    "(1) 非负性：$P(A | B) \\geq 0$\n",
    "\n",
    "(2) 规范性：$P(\\Omega | B) = 1$\n",
    "\n",
    "(3) 可列可加性：如果事件$A_{1}, A_{2}, \\dots$两两互不相容，则\n",
    "\n",
    "$$P(\\bigcup_{i = 0}^{\\infty} A_{i} | B) = \\sum_{i = 0}^{\\infty} P(A_{i} | B) \\tag {2-7}$$\n",
    "\n",
    "$A_{i}$和$A_{j}$条件独立，当且仅当\n",
    "\n",
    "$$P(A_{i}, A_{j} | B) = P(A_{i} | B) P(A_{j} | B)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 贝叶斯法则\n",
    "\n",
    "$$P(B | A) = \\frac{P(B \\bigcap A)}{P(A)} = \\frac{P(A | B)P(B)}{P(A)} \\tag{2-8}$$\n",
    "\n",
    "$$\\argmax_{B} P(B | A) = \\argmax_{B} \\frac{P(A | B)P(B)}{P(A)} = \\argmax_{B} P(A | B)P(B) \\tag{2-9}$$\n",
    "\n",
    "假设$B$是样本空间$\\Omega$的一个划分，即$\\sum_{i} B_{i} = \\Omega$。如果$A \\subseteq \\bigcup_{i} B_{i}$，并且$B_{i}$互不相交，则$A = \\bigcup_{i} B_{i}A$，$P(A) = \\sum_{i} P(B_{i}A)$。由乘法定理可得\n",
    "\n",
    "$P(A) = \\sum_{i} P(A | B_{i}) P(B_{i}) \\tag {2-10}$\n",
    "\n",
    "方程(2-10)称为全概率公式。\n",
    "\n",
    "\n",
    "贝叶一斯法则（贝叶斯理论，Bayesian theorem)：假设$A$为样本空间$\\Omega$的事件，$B_{1}, B_{2}, \\dots, B_{n}$为$\\Omega$的一个划分，如果$A \\subseteq \\bigcup_{i = 1}^{n} B_{i}$，$P(A) \\gt 0$，并且$\\forall i \\not = j$，$B_{i} \\bigcap B_{j} = \\emptyset$，$P(B_{i}) \\gt 0$（$i = 1, 2, \\dots, n$），则\n",
    "\n",
    "$$P(B_{j} | A) = \\frac{P(A | B_{j})P(B_{j})}{P(A)} = \\frac{P(A | B_{j})P(B_{j})}{\\sum_{i = 1}^{n} P(A | B_{i}) P(B_{i})} \\tag {2-11}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 随机变量\n",
    "\n",
    "随机变量（random variable）是随机试验结果的函数，设$X$为一离散型随机变量，其全部可能取值为$\\{a_{1}, a_{2}, \\dots\\}$，则\n",
    "\n",
    "$$p_{i} = P(X = a_{i}), \\ i = 1, 2, \\dots \\tag {2-12}$$\n",
    "\n",
    "称为$X$的概率函数（概率分布）。$p_{i} \\geq 0$，$\\sum_{i = 1} p_{i} = 1$。方程(2一12)也称随机变量$X$的概率分布，\n",
    "\n",
    "$$P(X \\leq x) = F(x), - \\infty \\leq x \\leq \\infty \\tag {2-13}$$\n",
    "\n",
    "\n",
    "称为$x$的分布函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 二项式分布\n",
    "\n",
    "假设某一事件$A$在一次试验中发生的概率为$p$，现把试验独立地重复进行$n$次。用变量$X$来表示$A$在这$n$次试验中发生的次数,则$X$的取值可能为$0, 1, \\dots, n$。\n",
    "\n",
    "由于$n$次试验相互独立，根据乘法定理：事件$X = i$的一个结果序列（$A \\bar{A }A A \\dots \\bar{A }A$）发生的概率为$p^{i} (1 - p)^{n-i}$。该序列的取值结果有$\\begin{pmatrix} n \\\\ i \\end{pmatrix}$种，因此\n",
    "\n",
    "$$p_{i} = \\begin{pmatrix} n \\\\ i \\end{pmatrix} p^{i} (1 - p)^{n-i}, \\ i = 1, 2, \\dots, n \\tag {2-14}$$\n",
    "\n",
    "$X$概率分布称为二项式分布（binomial distribution），记为$B(n, p)$。\n",
    "\n",
    "随机变量$X$服从分布$F$时，记为$X \\sim B(n, p)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7 联合概率分布和条件概率分布\n",
    "\n",
    "假设$(X_{1}, X_{2})$为一个二维的离散型随机向量，$X_{1}$的全部可能取值为$a_{1}, a_{2}, \\dots$；$X_{2}$的全部可能取值为$b_{1}, b_{2}, \\dots$，则$(X_{1}, X_{2})$的联合分布（joint distribution）为\n",
    "\n",
    "$$p_{ij} = P(X_{1} = a_{i}, X_{2} = b_{j}), \\ i = 1, 2, \\dots; j = 1, 2, \\dots$$\n",
    "\n",
    "考虑$X_{1}$在给定$X_{2} = b_{j}$条件下的概率分布，$P(X_{1} = a_{i} | X_{2} = b_{j})$，根据条件概率的定义可得\n",
    "\n",
    "$$P(X_{1} = a_{i} | X_{2} = b_{j}) = \\frac{P(X_{1} = a_{i}, X_{2} = b_{j})}{P(X_{2} = b_{j})} = \\frac{p_{ij}}{P(X_{2} = b_{j})}$$\n",
    "\n",
    "由于$P(X_{2} = b_{j}) = \\sum_{k} p_{kj}$，故有\n",
    "\n",
    "$$P(X_{1} = a_{i} | X_{2} = b_{j}) = \\frac{p_{ij}}{\\sum_{k} p_{kj}}, \\ i = 1, 2, \\dots \\tag {2-15}$$\n",
    "\n",
    "同理，\n",
    "\n",
    "$$P(X_{2} = b_{j} | X_{1} = a_{i}) = \\frac{p_{ij}}{\\sum_{k} p_{ik}}, \\ j = 1, 2, \\dots \\tag {2-16}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.8 贝叶斯决策理论\n",
    "\n",
    "贝叶斯决策理论（Bayesian decision theory）：假设分类问题有$c$个类别，各类别的状态用$w_{i}$表示，$i = 1, 2, \\dots, c$，各类别$w_{i}$的先验概率为$P(w_{i})$；给定$d$维特征空间中某一向量（点）$\\mathbf{x} = [x_{1}, x_{2}, \\dots, x_{d}]^{\\text{T}}$，且条件概率密度函数（似然，likelihood）$p(\\mathbf{x} | w_{i})$已知，则由贝叶斯公式可得后验概率$P(w_{i} | \\mathbf{x})$如下：\n",
    "\n",
    "$$P(w_{i} | \\mathbf{x}) = \\frac{p(\\mathbf{x} | w_{i}) P(w_{i})}{\\sum_{j = 1}^{c} p(\\mathbf{x} | w_{j}) P(w_{j})}$$\n",
    "\n",
    "基于最小错误率的贝叶斯斯决策准则为：\n",
    "\n",
    "1. 如果$P(w_{i} | \\mathbf{x}) = \\max_{j = 1, 2, \\dots, c} P(w_{i} | \\mathbf{x})$，则$\\mathbf{x} \\in w_{i}$，或者\n",
    "\n",
    "2. 如果$p(\\mathbf{x} | w_{i}) P(w_{i}) = \\max_{j = 1, 2, \\dots, c} p(\\mathbf{x} | w_{i}) P(w_{i})$，则$\\mathbf{x} \\in w_{i}$\n",
    "\n",
    "当$c = 2$时，若$l(\\mathbf{x}) = \\frac{p(\\mathbf{x} | w_{1})}{p(\\mathbf{x} | w_{2})} \\gt \\frac{P(w_{2})}{P(w_{1})}$，则$\\mathbf{x} \\in w_{1}$；反之，$\\mathbf{x} \\in w_{2}$。其中，$l(\\mathbf{x})$为似然比（likelihood ratio），而$\\frac{P(w_{2})}{P(w_{1})}$称为似然比域值（threshold）。\n",
    "\n",
    "\n",
    "贝叶斯决策理论可用于解决自然语言处理中的词义消歧（word sense disambiguation，WSL）、文本分类等问题。\n",
    "\n",
    "*最小风险的贝叶斯决策*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.9 期望和方差\n",
    "\n",
    "期望（expectation）是指随机变量所有取值的概率平均。假设$X$为一随机变量，其概率分布为$P(X = x_{k}) = p_{k}, \\ k = 1, 2, \\dots$，若级数$\\sum_{k = 1}^{\\infty} x_{k} p_{k}$绝对收敛，则随机变量$X$的数学期望或概率平均值为\n",
    "\n",
    "$$E(X) = \\sum_{k = 1}^{\\infty} x_{k} p_{k} \\tag {2-20}$$\n",
    "\n",
    "随机变量的方差（variance）描述该随机变童的值偏离其期望值的程度。$X$为一随机变量，则其方差$\\text{var}(X)$为\n",
    "\n",
    "$$\\text{var}(X) = E((X - E(x))^{2}) = E(X^{2}) - E^{2}(X) \\tag {2-21}$$\n",
    "\n",
    "平方根$\\sqrt{\\text{var}(X)}$称为$X$的标准差。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 信息论基本概念\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 熵\n",
    "\n",
    "如果$X$是个离散型随机变量，取值空间为$\\R$，其概率分布为$p(x) = P(X = x), x \\in \\R$。则$X$的熵$H(X)$定义为：\n",
    "\n",
    "$$H(X) = - \\sum_{x \\in \\R} p(x) \\log_{2} p(x) \\tag {2-22}$$\n",
    "\n",
    "其中，约定$0 \\log 0 = 0$，$H(X)$可记为$H(p)$\n",
    "\n",
    "熵又称为自信息（self-information），用于描述一个随机变量的不确定性，它表示信源$X$每发一个符号所提供的平均信息量。一个随机变量的熵越大，它的不确定性越大，则正确估计其值的可能性就越小。越不确定的随机变量越需要大的信息量用以确定其值。\n",
    "\n",
    "在已知部分知识的前提下，关于未知分布最合理的推断应该是符合已知知识最不确定或最大随机的推断，即使熵值最大的概率分布。\n",
    "\n",
    "最大熵概念在自然语言处理中的应用：通常做法是根据已知样本设计特征函数，假设存在$k$个特征函数了$f_{i} (i = 1, 2, \\dots, k)$，所建模型$p$应该属于这$k$个特征函数约束下所产生的所有模型的集合$C$。用熵$H(p)$值最大的模型推断某种语言现象存在的可能性，或者作为进行某种处理操作的可靠性依据，即：\n",
    "\n",
    "$$\\hat{p} = \\argmax_{p \\in C} H(p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 联合熵和条件熵\n",
    "\n",
    "如果$X, Y$是一对离散型随机变量$X, Y \\sim p(x, y)$，$X, Y$的联合熵（joint entropy）$H(X, Y)$定义为\n",
    "\n",
    "$$H(X, Y) = - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(x, y) \\tag {2-23}$$\n",
    "\n",
    "给定随机变量$X$的情况下，随机变量$Y$的条件嫡（conditional entropy）定义为：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "H(Y | X) & = \\sum_{x \\in X} p(x) H(Y | X = x) \\\\\n",
    "& = \\sum_{x \\in X} p(x) \\left[ - \\sum_{y \\in Y} p(y | x) \\log p(y | x) \\right] \\\\\n",
    "& = - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(y | x)\n",
    "\\end{aligned} \\tag {2-24}$$\n",
    "\n",
    "由方程(2-23)可知：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "H(X, Y) & = - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(x, y) \\\\\n",
    "& = - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log \\left[ p(x) p(y | x) \\right] \\\\\n",
    "& = - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\left[ log p(x) + \\log p(y | x) \\right] \\\\\n",
    "& = - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) log p(x) - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(y | x) \\\\\n",
    "& = H(X) + H(Y | X)\n",
    "\\end{aligned} \\tag {2-25}$$\n",
    "\n",
    "方程(2-25)称为熵的链式法则（chain rule for entropy）。推广到一般情况，有\n",
    "\n",
    "$$\\begin{aligned}\n",
    "H(X_{1}, X_{2}, \\dots, X_{n}) = H(X_{1}) + \\sum_{i = 2}^{n} H(X_{i} | X_{1}, \\dots, X_{i - 1})\n",
    "\\end{aligned}$$\n",
    "\n",
    "对于一条长度为$n$的信息，每一个字符或字的熵为\n",
    "\n",
    "$$H_{\\text{rate}} = \\frac{1}{n} H(X_{1}^{n}) = - \\frac{1}{n} \\sum_{x_{1}^{n}} p(x_{1}^{n}) \\log p(x_{1}^{n}) \\tag {2-26}$$\n",
    "\n",
    "该值称为熵率（entropy rate）。其中，变量$X_{1}^{n}$表示随机变量序列$(X_{1}, X_{2}, \\dots, X_{n})$，$x_{1}^{n} = (x_{1}, x_{2}, \\dots, x_{n})$。\n",
    "\n",
    "假定一种语言是由一系列符号组成的随机过程，$L = (X_{i})$，则可以定义这种语言$L$的熵作为其随机过程的熵率，即\n",
    "\n",
    "$$H_{\\text{rate}}(L) = \\lim_{n \\rightarrow \\infty} \\frac{1}{n} H(X_{1}, X_{2}, \\dots, X_{n}) \\tag {2-27}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 互信息\n",
    "\n",
    "根据熵的链式法则，$H(X, Y) = H(X) + H(Y | X) = H(Y) + H(X | Y)$，可知\n",
    "\n",
    "$$H(X) - H(X | Y) = H(Y) - H(Y | X)$$\n",
    "\n",
    "该差值称为$X$和$Y$的互信息（mutual information，MI），记作$I(X ; Y)$。或者定义为：如果$(X, Y) \\sim p(x, y)$，则$X$和$Y$之间的互信息$I(X ; Y) = H(X) - H(X | Y)$。$I(X ; Y)$反映了知道$Y$的值之后，$X$不确定性的减少量。\n",
    "\n",
    "<img src=\"./img/fig_2_1.jpg\" width=\"300\" />\n",
    "\n",
    "将$H(X)$和$H(X | Y)$展开，可得\n",
    "\n",
    "$$\\begin{aligned}\n",
    "I(X ; Y) & = H(X) - H(X | Y) \\\\\n",
    "& = H(X) + H(Y) - H(X, Y) \\\\\n",
    "& = - \\sum_{x \\in X} p(x) log p(x) - \\sum_{y \\in Y} p(y) log p(y) + \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(x, y) \\\\\n",
    "& = \\sum_{x, y} p(x, y) \\log \\frac{p(x, y)}{p(x) p(y)} \\\\\n",
    "\\end{aligned} \\tag {2-28}$$\n",
    "\n",
    "由于$H(X | X) = 0$，因此$H(X) = H(X) - H(X | X) = I(X ; X)$，故熵又称为自信息。两个完全相互依赖的变量之间的互信息并不是一个常量，而是取决于它们的熵。\n",
    "\n",
    "互信息体现了两变量之间的依赖程度：如果$I(X ; Y) \\gg 0$，表明$X$和$Y$是高度相关的；如果$I(X ; Y) = 0$，表明$X$和$Y$是相互独立的；如果$I(X ; Y = y) \\ll 0$，表明事件$Y = y$的出现不但未使$X$的不确定性减小，反而增大$X$的不确定性[^1]。平均互信息量是非负的，$I(X ; Y) \\geq 0$。\n",
    "\n",
    "条件互信息和互信息的链式法则：\n",
    "\n",
    "$$I(X ; Y | Z) = H(X | Z) -  H(X | Y, Z) \\tag {2-29}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "I(X_{1}^{n} ; Y) & = I(X_{1}, X_{2}, \\dots, X_{n} ; Y) \\\\\n",
    "& = I(X_{1} ; Y) + \\sum_{i = 2}^{n} I(X_{i} ; Y | X_{1}, \\dots, X_{i - 1})\n",
    "\\end{aligned} \\tag {2-30}$$\n",
    "\n",
    "互信息在词汇聚类（word clustering）、汉语自动分同、词义消歧等问题中具有重要用途。\n",
    "\n",
    "[^1]: 原文为如果$I(X ; Y = y) \\ll 0$，表明$Y$的出现不但未使$X$的不确定性减小，反而增大$X$的不确定性。此处应是为作者本意应为特定事件笔误，作者本意应为特定事件$Y = y$对$X$的影响，而非变量$Y$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 相对熵\n",
    "\n",
    "相对熵（relative entropy）又称Kullback-Leibler散度（Kullback-Leibler divergence），或简称KL距离，是衡量相同事件空间中两个概率分布相对差距的测度。两个概率分布$p(x)$和$q(x)$的相对熵定义为\n",
    "\n",
    "$$D(p \\| q) = \\sum_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)} \\tag {2-31}$$\n",
    "\n",
    "该定义中约定$0 \\log \\frac{0}{q} = 0$，$p \\log \\frac{p}{0} = \\infty$。表示成期望值为\n",
    "\n",
    "$$D(p \\| q) =E_{p} \\left(\\log \\frac{p(X)}{q(X)} \\right) \\tag {2-32}$$\n",
    "\n",
    "显然，当两个随机分布完全相同时，即$p = q$，其相对熵为0；当两个随机分布的差别增加时，其相对熵期望值也增大。\n",
    "\n",
    "互信息实际上就是衡量一个联合分布与独立性差距的测度：\n",
    "\n",
    "$$I(X ; Y) = D(p(x, y) \\| p(x) p(y)) \\tag {2-33}$$\n",
    "\n",
    "条件相对熵和相对熵的链式法则：\n",
    "\n",
    "$$D(p(y | x) \\| q(y | x)) = \\sum_{x} p(x) \\sum_{y} p(y | x) \\log \\frac{p(y | x)}{q(y | x)} \\tag {2-34}$$\n",
    "\n",
    "$$D(p(x, y) \\| q(x, y)) = D(p(x) \\| q(x)) + D(p(y | x) \\| q(y | x)) \\tag {2-35}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 交叉熵\n",
    "\n",
    "熵是不确定性的测度，交叉熵是用于衡量估计模型与真实概率分布之间的差异。如果一个随机变量$X \\sim p(x)$，$q(x)$为$p(x)$的近似概率分布，则，随机变量$X$和模型$q(x)$之间的交叉熵（cross entropy）定义为\n",
    "\n",
    "$$\\begin{aligned}\n",
    "H(X, q) & = H(X) + D(p \\| q) \\\\\n",
    "& = - \\sum_{x \\in X} p(x) \\log q(x) \\\\\n",
    "& = E_{p} \\left( \\log \\frac{1}{q(x)} \\right) \\\\\n",
    "\\end{aligned} \\tag {2-36}$$\n",
    "\n",
    "定义语言$L = (X_{i}) \\sim p(x)$与其模型$q$的交叉熵为\n",
    "\n",
    "$$\\begin{aligned}\n",
    "H(L, q) & = - \\lim_{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{x_{1}^{n}} p(x_{1}^{n}) \\log q(x_{1}^{n})\n",
    "\\end{aligned} \\tag {2-37}$$\n",
    "\n",
    "其中，$x_{1}^{n} = x_{1}, x_{2}, \\dots, x_{n}$表示$L$的语句，$p(x_{1}^{n})$为$L$中$x_{1}^{n}$的概率，$q(x_{1}^{n})$为模型对$x_{1}^{n}$的概率估计。\n",
    "\n",
    "真实概率$p(x_{1}^{n})$未知，假设语言$L$是“理想”的，即$n$趋于无穷大时，其全部“单词”的概率和为1。根据信息论的定理：假定语言$L$是平稳（stationary）各态历经（ergodic）的随机过程，$L$与其模型$q$的交叉熵为\n",
    "\n",
    "$$\\begin{aligned}\n",
    "H(L, q) & = - \\lim_{n \\rightarrow \\infty} \\frac{1}{n} \\log q(x_{1}^{n})\n",
    "\\end{aligned} \\tag {2-38}$$\n",
    "\n",
    "因此，可以根据模型$q$和一个含有大量数据的$L$的样本计算交叉熵。设计模型$q$时，目标是使交叉熵最小，使得模型最接近真实的概率分布$p(x)$。当$n$足够大时，可采用近似计算：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "H(L, q) & \\approx - \\frac{1}{n} \\log q(x_{1}^{n})\n",
    "\\end{aligned} \\tag {2-39}$$\n",
    "\n",
    "交叉熵与模型在测试语料中分配给每个单词的平均概率所表达的含义正好相反，模型的交叉熵越小，模型的表现越好。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 困惑度\n",
    "\n",
    "设计语言模型时，通常用困惑度（perplexity）代替交叉熵衡量语言模型。给定语言$L$的样本$l_{1}^{n} = l_{1}, \\dots, l_{n}$，$L$的困惑度$\\text{PP}_{q}$定义为\n",
    "\n",
    "$$\\text{PP}_{q} = 2^{H(L, q)} \\approx 2^{- \\frac{1}{n} \\log q(l_{1}^{n})} = [q(l_{1}^{n})]^{- \\frac{1}{n}} \\tag {2-40}$$\n",
    "\n",
    "语言模型的设计目标是寻找困惑度最小的模型，使其最接近真实语言的情况。在自然语言处理中，语言模型的困惑度通常指语言模型对于测试数据的困惑度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.7 噪声信道模型\n",
    "\n",
    "信息熵可以定量估计信源每发送一个符号所提供的平均信息量，但对于通信系统来说，最根本的问题还在于如何定量估算从信道输出中获取多少信息量。\n",
    "\n",
    "香农为模型化信道通信问题，以熵概念为基础，提出噪声信道模型（noisy channel model），其目标为优化噪声信道中信号传输的吞吐量和准确率，其基本假设是一个信道的输出以一定的概率依赖于输入。\n",
    "\n",
    "<img src=\"./img/fig_2_2.jpg\" width=\"500\" />\n",
    "\n",
    "二进制对称信道（binary symmetric channel，BSC）的输入符号集为$X = \\{0, 1\\}$，输出符号集为$Y = \\{0, 1\\}$。在传输过程中，如果输入符号被误传的概率为$p$，则，正确传输的概率为$1 - p$。\n",
    "\n",
    "<img src=\"./img/fig_2_3.jpg\" width=\"200\" />\n",
    "\n",
    "信道容量（capacity）定义为最大化互信息：\n",
    "\n",
    "$$C = \\max_{p(X)} I(X ; Y) \\tag {2-41}$$\n",
    "\n",
    "其基本思想为降低传输速率以换取高保真通信的可能性。根据该定义，如果能够设计一个输入编码$X$，其概率分布为$p(X)$，使其输入与输出之间的互信息达到最大值，则该设计达到了信道的最大传输容量。\n",
    "\n",
    "自然语言处理中，一种自然语言的句子可视为已编码的符号序列，通过解码，使观察到的输出序列更接近输入序列，因此，可用噪声信道模型描述。\n",
    "\n",
    "<img src=\"./img/fig_2_4.jpg\" width=\"400\" />\n",
    "\n",
    "根据贝叶斯公式，\n",
    "\n",
    "$$\\hat{I} = \\argmax_{I} p(I | O) = \\argmax_{I} \\frac{p(O | I) p(I)}{P(O)} = \\argmax_{I} p(O | I) p(I) \\tag {2-42}$$\n",
    "\n",
    "方程(2-42)中，$p(I)$为语言模型（language model），是指在输入语言中“词”序列的概率分布；$p(O | I)$为信道概率（channel probability）。\n",
    "\n",
    "在自然语言处理中，噪声信道模型主要用于机器翻译、词性标注、语音识别、文字识别等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 支持向量机\n",
    "\n",
    "支持向量机（support vector machinc，SVM）是在高维特征空间使用线性函数假设空间的学习系统，在分类方面具有良好的性能。在自然语言处理中，SVM广泛应用于短语识别、词义消歧、文本自动分类和信息过滤等方面。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 线性分类\n",
    "\n",
    "二分类问题通常用实数函数$f : X \\subseteq \\R^{n} \\rightarrow \\R$（$n$为输入维数）判别：当$f(\\mathbf{x}) \\geq 0$时，将输入$\\mathbf{x} = (x_{1}, x_{2}, \\dots, x_{n})^{\\text{T}}$判为正类；否则，为负类。当$f(\\mathbf{x})$（$\\mathbf{x} \\in X$）是线性函数时，$f(\\mathbf{x})$可写成如下形式：\n",
    "\n",
    "$$f(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle + b = \\sum_{i = 1}^{n} w_{i} x_{i} + b \\tag {2-43}$$\n",
    "\n",
    "其中，$(\\mathbf{w}, b) \\in \\R^{n} \\times \\R$是控制函数的参数，决策规则由符号函数$\\text{sgn}(f(\\mathbf{x}))$给出，通常$\\text{sgn}(0) = 1$。参数学习意味着要从训练数据中获得这此参数。\n",
    "\n",
    "该分类方法的几何解释是：方程$\\langle \\mathbf{w}, \\mathbf{x} \\rangle + b = 0$定义的超平面将输入空间$X$分成两半，一半为负类，一半为正类，\n",
    "\n",
    "<img src=\"./img/fig_2_5.jpg\" width=\"350\" />\n",
    "\n",
    "图中黑斜线表示超平面，对应地，超平面上面为正区域，用符号`+`表示，下面为负区域，用符号`-`表示。$\\mathbf{w}$是超平面的法线方向。当$b$的值变化时，超平面平行移动。因此，如果要表达$\\R^{n}$中所有可能的超平面，一般要包括$n + 1$个可调参数的表达式。\n",
    "\n",
    "如果训练数据线性，则以最大间隔分开数据的超平面称为最优超平面，\n",
    "\n",
    "<img src=\"./img/fig_2_6.jpg\" width=\"350\" />\n",
    "\n",
    "对于多类分类问题，输出域是$Y = \\{ 1, 2, \\dots, m \\}$，线性学习器推广到$m$（$m \\in N, m \\geq 2$）类问题：对于$m$类中的每一类关联一个权重向量$\\mathbf{w}_{i}$和偏置$b_{i}$，即$(\\mathbf{w}_{i}, b_{i})$，$i \\in \\{1, 2, \\dots, m\\}$，定义决策函数：\n",
    "\n",
    "$$c(\\mathbf{x}) = \\argmax_{1 \\leq i \\leq m} \\langle \\mathbf{w}_{i}, \\mathbf{x} \\rangle + b_{i} \\tag {2-44}$$\n",
    "\n",
    "其几何意义是：给每个类关联一个超平面，然后将新点$\\mathbf{x}$赋予超平面离其最远的那一类。输入空间被划分为$m$个简单相连的凸区域。\n",
    "\n",
    "P.S.：方程（2-44）相当于`one-verse-rest`方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 线性不可分\n",
    "\n",
    "对于非线性问题，可以把样本$\\mathbf{x}$映射到某个高维特征空间，在高维特征空间中使用线性学习器。因此，假设集是如下类型函数：\n",
    "\n",
    "$$f(\\mathbf{x}) = \\langle \\mathbf{w}, \\varphi(\\mathbf{x}) \\rangle + b \\tag {2-45}$$\n",
    "\n",
    "其中，$\\varphi: X \\rightarrow F$表示从输人空间$X$到特征空间$F$的映射。即建立非线性分类器分为两步：首先使用一个非线性映射函数将数据变换到一个特征空间$F$，然后在这个特征空间上使用线性分类器。\n",
    "\n",
    "*线性分类器的一个重要性质是可以表示成对偶形式，这意味着假设可以表达为训练点的线性组合。* 因此，决策规则（分类函数）可以用测试点和训练点的内积来表示：\n",
    "\n",
    "$$f(\\mathbf{x}) = \\sum_{i = 1}^{l} \\alpha_{i} y_{i} \\langle \\varphi(\\mathbf{x}_i), \\varphi(\\mathbf{x}) \\rangle + b \\tag {2-46}$$\n",
    "\n",
    "其中，$l$是样本数目；$\\alpha_{i}$为正值导数，可通过学习获得；$y_{i}$为样本$i$的类别标签。如果有一种方法可以在特征空间中直接计算内积$\\langle \\varphi(\\mathbf{x}_i), \\varphi(\\mathbf{x}) \\rangle$，就像在原始输入点的函数中一样，则有可能将两个步骤融合到一起，建立一个非线性分类器。在高维空间中实际上只需要进行内积运算，而这种内积运算是可以利用原空间中的函数实现的，我们甚至没有必要知道变换的形式。这种直接计算的方法称为核（kernel）函数方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 构造核函数\n",
    "\n",
    "定义2-1：核是一个函数$K$，对所有$\\mathbf{x}, \\mathbf{z} \\in X$,满足\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{z}) = \\langle \\varphi(\\mathbf{x}), \\varphi(\\mathbf{z}) \\rangle \\tag {2-47}$$\n",
    "\n",
    "其中，$\\varphi$表示从输入空间$X$到特征空间$F$的映射。有了核函数，决策规则就可以通过对核函数的$l$次计算得到：\n",
    "\n",
    "$$f(\\mathbf{x}) = \\sum_{i = 1}^{l} \\alpha_{i} y_{i} K(\\mathbf{x}_{i}, \\mathbf{x}) + b \\tag {2-48}$$\n",
    "\n",
    "这种方法的关键是如何找到一个可以高效计算的核函数。\n",
    "\n",
    "为适合某个特征空间，核函数必须是对称的，即\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{z}) = \\langle \\varphi(\\mathbf{x}), \\varphi(\\mathbf{z}) \\rangle = \\langle \\varphi(\\mathbf{z}), \\varphi(\\mathbf{x}) \\rangle = K(\\mathbf{z}, \\mathbf{x}) \\tag {2-49}$$\n",
    "\n",
    "并且满足柯西不等式：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "K^{2}(\\mathbf{x}, \\mathbf{z})\n",
    "& = \\langle \\varphi(\\mathbf{x}), \\varphi(\\mathbf{z}) \\rangle^{2} \\\\\n",
    "& \\leq \\| \\varphi(\\mathbf{x})\\|^{2} \\| \\varphi(\\mathbf{z}) \\|^{2} \\\\\n",
    "& = \\langle \\varphi(\\mathbf{x}), \\varphi(\\mathbf{x}) \\rangle^{2} \\langle \\varphi(\\mathbf{z}), \\varphi(\\mathbf{z}) \\rangle^{2} \\\\\n",
    "& = K^{2}(\\mathbf{x}, \\mathbf{x}) K^{2}(\\mathbf{z}, \\mathbf{z})\n",
    "\\end{aligned} \\tag {2-50}$$\n",
    "\n",
    "其中，$\\| \\cdot \\|$表示欧氏模函数。但这些条件对于保证特征空间的存在是不充分的，还必须满足`Mercer`定理的条件：对$X$的任意有限子集，相应的矩阵是半正定的。即令$X$是有限输入空间，$K(\\mathbf{x}, \\mathbf{z})$是$X$上的对称函数，则$K(\\mathbf{x}, \\mathbf{z})$是核函数的充分必要条件为矩阵\n",
    "\n",
    "$$\\mathbf{K} = \\left( K(\\mathbf{x}_{i}, \\mathbf{x}_{j}) \\right)_{i, j = 1}^{n} \\tag {2-51}$$\n",
    "\n",
    "是半正定的（即特征值非负）。\n",
    "\n",
    "根据泛函的有关理论，只要一种核函数满足`Mercer`条件，它就对应某一空间中的内积。\n",
    "\n",
    "支持向量机中常用的核函数主要有：多项式核函数、径向基函数、多层感知机、动态核函数等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
